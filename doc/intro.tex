\section{Introduction}

This document contains the mathematical formalizations
for the decision tree learning-classification composition
DSL and semantics.
Specifically, we will consider changes to the training set of the form of ``dropping''
elements from the set:
if I am allowed to remove $n$ elements from a training set $T$,
what possible posteriors could I now obtain by retraining a decision tree model
and reclassifying some input $x$?
For now, we restrict ourselves to binary classification,
so training sets are $T \subseteq \mathcal{X} \times \{0, 1\}$ for some feature space $\mathcal{X}$
and posteriors can be represented by a single real number $p \in [0, 1]$.
Additionally, we will only consider finite sets of predicates
$\Phi = \{\varphi_i\}_i$ where each
$\varphi_i : \mathcal{X} \rightarrow \{\mathit{true}, \mathit{false}\}$.
