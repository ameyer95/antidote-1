\section{A Primitive Language}\label{sec:dsl}

Decision trees inductively, greedily split the training set
using some set of predicates based on the feature set.
A test input is then classified by following a single path through the tree.
Note then, that since we care about the composition of training the tree
and classifying a single input,
that we need not actually construct the full tree---%
we need only to construct the path that the specified input would follow
(as if we were training the tree depth-first and stopping after a single path).
We provide a (very minimal) language in which the training-classification
composition can be written;
each program has a fixed test input $x$ and (finite) set of predicates $\Phi$
(totally ordered by some given $<_\Phi$)
and additionally manipulates a state consisting of a triple $(T, \varphi, p)$.
Here, $T$ is the current slice of the training set,
$\varphi$ is the most recent predicate chosen for splitting,
and $p$ stores the posterior to be returned (the classification value).
\todo{The language does not exactly match the ASTNode class in the implementation.}

\begin{figure}
\begin{minipage}{0.65\textwidth}
\centering
\begin{tabular}{rcl}
Program $P$ & $\coloneqq$ & $S ; R$ \\
Statements $S$ & $\coloneqq$ & $S_1 ; S_2$ \\
& $\mid$ & $\iteimpuritynode{S_1}{S_2}$ \\
& $\mid$ & $\itetrivialnode{S}{U}$ \\
& $\mid$ & $\bestsplitnode$ \\
& $\mid$ & $\summarynode$ \\
Use-$\varphi$ Statements $U$ & $\coloneqq$ & $S$ \\
& $\mid$ & $U_1 ; U_2$ \\
& $\mid$ & $\itemodelsnode{U_1}{U_2}$ \\
& $\mid$ & $\filternode$ \\
& $\mid$ & $\filternode[\lnot]$ \\
Return Statement $R$ & $\coloneqq$ & $\returnnode$ \\
\end{tabular}
\end{minipage}
%
\begin{minipage}{0.3\textwidth}
\centering
\begin{tabular}{l}
if $\mathit{impurity}(T) = 0$ then \\
\quad $p \gets \mathit{summary}(T)$ \\
else \\
\quad $\varphi \gets \mathit{bestsplit}_\Phi(T)$ \\
\quad if $\varphi = \bot$ then \\
\qquad $p \gets \mathit{summary}(T)$ \\
\quad else \\
\qquad if $x \models \varphi$ then \\
\qquad\quad $T \gets \mathit{filter}(T, \varphi)$ \\
\qquad else \\
\qquad\quad $T \gets \mathit{filter}(T, \lnot\varphi)$ \\
\qquad $p \gets \mathit{summary}(T)$ \\
return $p$
\end{tabular}
\end{minipage}
\caption{Left: Simple inductive splitting language.
Right: Depth-1 classifier.}
\label{fig:dsl}
\end{figure}

The grammar is given in Figure~\ref{fig:dsl} (left).
``Statements'' $S$ and ``Use-$\varphi$ Statements'' $U$ are divided
to statically enforce that $\varphi \neq \bot$ whenever, well,
we need it to be.
Accordingly, every Statement is a Use-$\varphi$ Statement,
but Use-$\varphi$ Statements can only be introduced in the else-branch
of ``if $\varphi = \bot$ \ldots'';
a Program $P$ must begin with a Statement,
not a Use-$\varphi$ statement.

The intended instantiations of the language correspond
to classifiers of specified depths;
for example, Figure~\ref{fig:dsl} (right) shows an implementation of
the training-classification composition for a tree of depth 1
(i.e.\ a single split).
It begins with ``if $\mathit{impurity}(T) = 0 \ldots$''
which checks if the current training set fragment has 100\% confidence
for one particular classification;
if so, it simply sets the posterior variable appropriately.
Otherwise, it tries to split the data up in some meaningful way
to give a better prediction:
this is done (greedily) by selecting a predicate $\varphi \in \Phi$
that has the highest \emph{mutual information} with the classification label
(ties are broken using $<_\Phi$-minimality):
if every predicate is a trivial split (i.e., yields $T \sqcup \emptyset$)
then we simply set the posterior appropriately;
otherwise, the training set is then restricted to the subset
that ``looks the same'' as the test input under $\varphi$
(using ``$T \gets \mathit{filter}(T, (\lnot)\varphi)$'').
Because this is a depth-1 tree, we have reached max depth
(the depth describes the number of splits, i.e.\ the number of
chosen $\varphi$ and $\mathit{filter}$ calls)
and use the current training set fragment to produce a posterior classification;
for greater max depths, we would repeat the prior block of code
in place of the last $p \gets \mathit{summary}(T)$ statement.
