\section{Illustrating the Challenge with Generalization}

We have instances of a number of types being manipulated:
\begin{itemize}
    \item A training set $T \in \mathcal{T}$
        (where $\mathcal{T}$ is some multiset of elements in $\mathcal{X} \times \mathcal{Y}$),
    \item A predicate $\varphi \in \Phi$
        (actually, we really have an option type,
        but I will just write $\varphi \in \Phi \cup \{\bot\}$ instead), and
    \item A posterior distribution (to be returned) $p \in D$.
\end{itemize}

In general, the program state is some
$\sigma = (T, \varphi, p) \in \mathcal{T} \times (\Phi \cup \{\bot\}) \times D$.
To make things concrete, we'll fix binary features and classification
$T \subset \{0,1\}^n \times \{0,1\}$,
Boolean predicates $\Phi = \{\lambda x \ldotp x[i] : 0 \leq i < n\}$,
and Bernoulli posterior distributions represented by $p \in [0, 1]$.
We will also focus on semantics for only two of the instructions in the DSL.
The first, $\summarynode$, needs to assign to $p$ a distribution over $\mathcal{Y}$
estimated from the current training set $T$;
the second, $\filternode$, needs to update the current training set $T$
based on the current predicate $\varphi$,
specifically by removing all elements $(x,y) \in T$ such that $x \not \models \varphi$.
Accordingly, we can write their concrete semantics as follows:
\begin{align*}
    \den{\summarynode}(\sigma) \coloneqq&~
    (T, \varphi, \frac{\lvert \{(x,y) \in T : y = 1\} \rvert}{\lvert T \rvert}) \\
    \den{\filternode}(\sigma) \coloneqq&~
    (\{(x,y) \in T : x \models \varphi\}, \varphi, p)
\end{align*}

Great. Now let's talk about doing some kind of abstract semantics.
Suppose we're going to ``hard-code'' it for the case of ``boxes,''
where each element in the state tuple is abstracted individually.
Furthermore, each element of the tuple will have a hard-coded abstraction;
we will have $\sigma^\# = (\langle T, n \rangle, \Psi, I)$ where:
\begin{itemize}
    \item For now, we will focus on considering the possible training sets formed
        by removing up to $n$-many elements from $T$.
        We represent this as $\langle T, n \rangle
        \coloneqq \{T' \subseteq T : \lvert T \setminus T' \rvert \leq n\}$.
    \item By earlier assumptions, $\Phi$ is finite, so we will consider
        any subset of predicates $\Psi \subseteq \Phi \cup \{\bot\}$
        represented as that actual subset.
    \item Individual posteriors were represented by a single real number in the unit interval,
        so a set of possible posteriors will be represented
        as some (single, closed) interval $I \subseteq [0, 1]$.
\end{itemize}
Since each of these components is hard-coded, we can hard-code the abstract semantics,
specifically using the knowledge that the state abstraction is a simple tuple,
that $\Psi$ is a finite set of predicates,
and that $n$ elements could be missing from $T$:
\begin{center}
\begin{align*}
    \den{\summarynode}^\#(\langle T, n \rangle, \Psi, I) \coloneqq&~
    (\langle T, n \rangle, \Psi, \mathit{summary}^\#(\langle T, n \rangle)) \\
    \den{\filternode}^\#(\langle T, n \rangle, \Psi, I) \coloneqq &~
    (\mathit{filter}^\#(\langle T, n \rangle, \Psi), \Psi, I)
\end{align*}
where
\begin{align*}
    \mathit{summary}^\#(\langle T, n \rangle) \coloneqq&~
    \text{let } c_0 = \lvert \{(x,y) \in T : y = 0\} \rvert
    \text{ and let } c_1 = \lvert \{(x,y) \in T : y = 1\} \rvert
    \text{ in} \\
    &~ \left[
    \frac{\max(c_1 - n, 0)}{c_0 + c_1 - n},
    \frac{c_1}{\max(c_0 - n, 0) + c_1} \right] \\
    \mathit{filter}^\#(\langle T, n \rangle, \Psi) \coloneqq&~
    \bigsqcup_{\varphi \in \Psi} \langle \{(x,y) \in T : x \models \varphi\}, n \rangle
\end{align*}
\end{center}
Note that the definition of the auxiliary $\mathit{filter}^\#$
requires us to have a join operation for the training set abstractions---this is ``fine,''
but the details are not important for now.

Now suppose we wanted to be more general!
We have some abstract domain $\mathcal{L}$ over program states,
where $\sigma^\# \in \mathcal{L}$ represents set of states
$\gamma(\sigma^\#) \subseteq \mathcal{T} \times (\Phi\cup\{\bot\}) \times D$.
$\mathcal{L}$ will have to implement a number of primitive transformers
as determined by writing the general abstract semantics:
\begin{align*}
    \den{\summarynode}^\#(\sigma^\#) \coloneqq&~ \mathit{applysummary}(\sigma^\#) \\
    \den{\filternode}^\#(\sigma^\#) \coloneqq&~ \mathit{applyfilter}(\sigma^\#)
\end{align*}
These are not very interesting since each is responsible just for assignments
(the if-then-else statements are more involved in the obvious way),
but accordingly, any state abstract domain must implement
$\mathit{applysummary}$ and $\mathit{applyfilter}$.

We can provide a general framework for boxes:
Suppose we have abstract domains $\mathcal{L}_\mathcal{T}$,
$\mathcal{L}_{\Phi \cup \{\bot\}}$, and $\mathcal{L}_D$
for abstractions over the training sets, predicates, and posterior distributions, respectively.
We then create $\mathcal{L}$ from their cartesian product, where $\sigma^\# = (T^\#, \varphi^\#, p^\#)$.
We can try to deal with each component of the state tuple, separately,
but we will soon see that some joint knowledge between domains is necessary:
\begin{align*}
    \mathit{applysummary}(\sigma^\#) \coloneqq&~ (T^\#, \varphi^\#, \mathit{summary}^\#(T^\#)) \\
    \mathit{applyfilter}(\sigma^\#) \coloneqq&~ (\mathit{filter}^\#(T^\#, \varphi^\#), \varphi^\#, p^\#)
\end{align*}
Observe that:
\begin{itemize}
    \item In order to implement $\mathit{summary}^\# : \mathcal{L}_\mathcal{T} \rightarrow \mathcal{L}_D$,
        we need knowledge of both $\mathcal{L}_\mathcal{T}$ and $\mathcal{L}_D$.
    \item In order to implement $\mathit{filter}^\# :
        \mathcal{L}_\mathcal{T} \times \mathcal{L}_{\Phi\cup\{\bot\}} \rightarrow \mathcal{L}_{\Phi\cup\{\bot\}}$
        we need knowledge of both $\mathcal{L}_\mathcal{T}$ and $\mathcal{L}_{\Phi\cup\{\bot\}}$.
\end{itemize}
We can use the previous work of the hard-coded domains to instantiate these
for $\sigma^\# = (\langle T, n \rangle, \Psi, I)$ (it's exactly the math written prior).
In general, we can leave it to the user to provide appropriate implementations of these
for whatever domains that they would want to be using
(which may be more complicated for reasons of precision, expressivity, \ldots).

Next, we want a disjunctive abstract state, based on the individual boxes,
to increase the precision of the analysis.
This will especially help with the joins after if-then-else statements.
Given some abstract domain over the states $\mathcal{L}$,
we will define a new abstract domain over the states $\mathcal{L}^k$
representing up to $k$-many disjuncts of whatever state abstraction $\mathcal{L}$ gives us.
If $\sigma^\# \in \mathcal{L}^k$ is denoted as $\{\sigma^\#_i\}_{1 \leq i \leq k}$
where each $\sigma^\#_i \in \mathcal{L}$
(which could include bottom elements),
then implementing the necessary functions is simple
by utilizing $\mathcal{L}$'s implementations:
\begin{align*}
    \mathit{applysummary}(\sigma^\#) \coloneqq&~
    \{\mathit{applysummary}(\sigma^\#_i) : 1 \leq i \leq k\} \\
    \mathit{applyfilter}(\sigma^\#) \coloneqq&~
    \{\mathit{applyfilter}(\sigma^\#_i) : 1 \leq i \leq k\}
\end{align*}
An interesting detail involves how to implement the join operation for $\mathcal{L}_k$,
but it is not relevant to this discussion.

\paragraph{Problem 1}
A big source of imprecision in the implementation of
$\mathit{filter}^\#$ for $\langle T, n \rangle$ and $\Psi$
involves the $\bigsqcup_{\varphi \in \Psi}$---%
if there are predicates in $\Psi$ that are extensionally very different,
we perform a very imprecise join.
This means that the prior $\mathcal{L}^k$ construction
will not benefit this loss of precision.

We can be more specific with an example:
suppose that the hard-coded $\mathit{filter}^\#$
was invoked when there were two possible predicates, and it is thus going to result in
$\langle T, n \rangle \sqcup \langle S, m \rangle
\coloneqq \langle T \cup S, \max(|T \setminus S| + m, |S \setminus T| + n) \rangle$.
Suppose that $|T| = |S| = 3k$ (for some $k$),
that $|\{(x,y) \in T : y = 1\}| = |\{(x,y) \in S : y = 1\}| = 2k$,
and that $T \cap S = \emptyset$.
Suppose further than $m = n = k$.
This gives us that $\langle T, n \rangle \sqcup \langle S, m \rangle
= \langle T \cup S, 4k \rangle$ (where $|T \cup S| = 6k$).
Then each individual component of the join has
$\mathit{summary}^\#(\langle T, n \rangle) = [\frac{1}{2}, 1]$,
and yet
$\mathit{summary}^\#(\langle T, n \rangle \sqcup \langle S, m \rangle)
= [0, 1]$.

Note that we \emph{do} want the precision increase of $\mathcal{L}^k$
for other parts of the semantics:
specifically, the joint $(T, \varphi)$ information is very important.
The problem is that disjuncts need to be introduced in more places
(like in $\mathit{filter}^\#$), but it's not clear the best way to do that.
We could consider somehow modifying the $\mathit{filter}^\#$ operation in $\mathcal{L}$
to ``be aware'' that it's operating as a subroutine of $\mathcal{L}^k$.
It would then have some heuristic by which it splits itself into multiple disjuncts.
This is not a nicely encapsulated, compositional approach.
Another solution could be to implement disjuncts \emph{both}
at the level of the whole state and at the level of the training set,
but it's easy to imagine that the training-set-level disjuncts could be distributed
very suboptimally between the state-level disjuncts.

\paragraph{Problem 2}
Because there is a lot of cross-dependencies between the constituent abstract domains
(e.g.\ the need of knowledge of both $\mathcal{L}_\mathcal{T}$ and $\mathcal{L}_D$
in order to implement $\mathit{summary}^\#(T^\#)$),
it is difficult to make small improvements to expressivity.

As an example,
currently, we hardcoded $\Psi$ so that each $\varphi$ looks like $\lambda x \ldotp x[i]$---%
in other words, we can identify $\varphi$ with a single integer index.
The obvious way to handle the infinitely-many predicates for real-valued data
is to have $\varphi$ identify with $(i, \ell, u)$ and look like
$\lambda x \ldotp x[i] \leq [\ell, u]$ (which is a three-valued predicate).
If we change $\mathcal{L}_{\Phi\cup\{\bot\}}$ very much,
it means needing to reimplement a lot of the primitives;
if we make such a drastic change,
it is potentially very non-trivial to implement $\mathit{filter}^\#$,
especially if $T^\#$ is also less convenient to work with.
The fact that $\mathit{filter}^\#$ must be implemented for every \emph{pair}
of $\mathcal{L}_\mathcal{T}$ and $\mathcal{L}_{\Phi\cup\{\bot\}}$
is very inconvenient.

In particular, the most complicated (and important) part of this whole system
is the $\mathit{bestsplit}_\Phi : \mathcal{T} \rightarrow \Phi \cup \{\bot\}$ computation's
corresponding abstract transformer.
This becomes increasingly difficult to work with as $\mathcal{L}_\mathcal{T}$
and $\mathcal{L}_{\Phi\cup\{\bot\}}$ become more complex,
and the fact that it must be rewritten for every pair of domains%
\footnote{Unless it can be generalized for special cases of those domains?
Like maybe when $\varphi^\#$ is represented as a finite set
that can be tractably enumerated \ldots}
is nasty.
